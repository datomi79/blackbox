{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import time : 7.14367413520813 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Author contributed : LIN HSUAN YU\n",
    "This script is developed and tested under the following environment:\n",
    "- Tensorflow-GPU : 1.3.0\n",
    "- Keras : 2.0.7 \n",
    "- Python : 3.5.2\n",
    "- CUDA SDK : 8.0\n",
    "- cudnn : 5.1\n",
    "\"\"\"\n",
    "import time\n",
    "t0 = t1 = time.time()\n",
    "from input_dataset import read_hr_dataset\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Dense, Activation\n",
    "import random, h5py\n",
    "t2 = time.time()\n",
    "random.seed(1234567890)\n",
    "print (\"Import time :\", t2-t1, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape :  (14999, 19)\n",
      "X_SCHEME :  ['satisfaction_level', 'last_evaluation', 'number_project', 'average_montly_hours', 'time_spend_company', 'Work_accident', 'promotion_last_5years', 'salary', 'IT', 'RandD', 'accounting', 'hr', 'management', 'marketing', 'product_mng', 'sales', 'support', 'technical']\n",
      "Y_SCHEME :  ['left']\n",
      "X_TRAIN :  (12000, 18)\n",
      "Y_TRAIN :  (12000, 1)\n",
      "X_TEST :  (2999, 18)\n",
      "Y_TEST :  (2999, 1)\n",
      "Data reading time : 0.10580301284790039 seconds\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "X_TRAIN, Y_TRAIN, X_TEST, Y_TEST = read_hr_dataset()\n",
    "t2 = time.time()\n",
    "print (\"Data reading time :\", t2-t1, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set keras learning phase and backend\n",
    "keras.layers.core.K.set_learning_phase(1)\n",
    "config = tf.ConfigProto(device_count = {'GPU' : 0})\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oracle Construction and Compilation time : 0.04521822929382324 seconds\n"
     ]
    }
   ],
   "source": [
    "# Construct the oracle with a keras model : 3 FC-layer\n",
    "t1 = time.time()\n",
    "oracle = Sequential()\n",
    "oracle.add(Dense(1000, input_dim = 18))\n",
    "oracle.add(Activation('relu'))\n",
    "oracle.add(Dense(100))\n",
    "oracle.add(Activation('sigmoid'))\n",
    "oracle.add(Dense(1))\n",
    "oracle.add(Activation('sigmoid'))\n",
    "\n",
    "# Compile the oracle model for binary classification\n",
    "oracle.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "t2 = time.time()\n",
    "print (\"Oracle Construction and Compilation time :\", t2-t1, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oracle Training time : 287.8393852710724 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train the oracle model (epoch = 1000)\n",
    "t1 = time.time()\n",
    "oracle.fit(X_TRAIN, Y_TRAIN, epochs=1000, batch_size=64, verbose=0)\n",
    "t2 = time.time()\n",
    "print (\"Oracle Training time :\", t2-t1, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved oracle configuration to disk\n",
      "Saved oracle weights to disk\n",
      "Oracle saving time : 0.019040822982788086 seconds\n"
     ]
    }
   ],
   "source": [
    "# Saved oracle configuration and weights to disk with json and hdf respectively\n",
    "t1 = time.time()\n",
    "print(\"Saved oracle configuration to disk\")\n",
    "oracle_json = oracle.to_json()\n",
    "with open(\"model/oracle.json\", \"w\") as oracle_file:\n",
    "    oracle_file.write(oracle_json)\n",
    "\n",
    "print(\"Saved oracle weights to disk\")\n",
    "oracle.save_weights(\"model/oracle_weights.h5\")\n",
    "t2 = time.time()\n",
    "print (\"Oracle saving time :\", t2-t1, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the oracle on the train set : 0.967916666667\n"
     ]
    }
   ],
   "source": [
    "# Output of the train set from the oracle model\n",
    "output_train = oracle.predict(X_TRAIN, batch_size=12000, verbose=0)\n",
    "output_train = np.array([1 if num >= 0.5 else 0 for num in output_train]).reshape(-1,1)\n",
    "correct_list = np.array([1 if output == y else 0 for output,y in zip(output_train,Y_TRAIN)])\n",
    "print(\"Accuracy of the oracle on the train set :\", sum (correct_list)/12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Substitute Construction time : 0.017919540405273438 seconds\n"
     ]
    }
   ],
   "source": [
    "# Construct a substitute model with 4-layer FC model via Keras\n",
    "t1 = time.time()\n",
    "sub = Sequential()\n",
    "sub.add(Dense(200, input_dim = 18))\n",
    "sub.add(Activation('sigmoid'))\n",
    "sub.add(Dense(1))\n",
    "sub.add(Activation('linear'))\n",
    "t2 = time.time()\n",
    "print (\"Substitute Construction time :\", t2-t1, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Generation time : 0.014447689056396484 seconds\n",
      "[ 213.]\n"
     ]
    }
   ],
   "source": [
    "# Select initial training samples from original test set for the substitute (sample_size=1000)\n",
    "t1, sample_size = time.time(), 1000\n",
    "while True :\n",
    "    sub_sample_x = np.array(random.sample (list(X_TEST), sample_size))\n",
    "    output_sample = oracle.predict(sub_sample_x, batch_size = sample_size, verbose = 0)\n",
    "    output_sample = np.array([1.0 if num >= 0.5 else 0.0 for num in output_sample]).reshape(-1,1)\n",
    "    if (sum(output_sample) > 0.2*sample_size and sum(output_sample) < 0.8*sample_size) or time.time()-t1 > 10 :\n",
    "        break\n",
    "t2 = time.time()\n",
    "print (\"Sample Generation time :\", t2-t1, \"seconds\")\n",
    "print (sum(output_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================== Cycle 0 started. ======================\n",
      "Batch size : 1000\n",
      "loss : 0.297023\n",
      "******** Epoch 1000 took 2 seconds. ********\n",
      "loss : 0.0655865\n",
      "******** Epoch 2000 took 2 seconds. ********\n",
      "loss : 0.0500141\n",
      "******** Epoch 3000 took 2 seconds. ********\n",
      "loss : 0.170078\n",
      "******** Epoch 4000 took 2 seconds. ********\n",
      "loss : 0.132528\n",
      "******** Epoch 5000 took 3 seconds. ********\n",
      "loss : 0.195815\n",
      "******** Epoch 6000 took 2 seconds. ********\n",
      "loss : 0.0145753\n",
      "******** Epoch 7000 took 3 seconds. ********\n",
      "loss : 0.0119146\n",
      "******** Epoch 8000 took 2 seconds. ********\n",
      "loss : 0.0107481\n",
      "******** Epoch 9000 took 2 seconds. ********\n",
      "loss : 0.00983815\n",
      "******** Epoch 10000 took 3 seconds. ********\n",
      "##### Cycle 0 : 13 seconds for sample generation. #####\n",
      "1000/1000 [==============================] - 0s\n",
      "# Real loss : 0.379975\n",
      "====================== Cycle 0 took 42 seconds. ======================\n",
      "====================== Cycle 1 started. ======================\n",
      "Batch size : 2000\n",
      "loss : 0.00776364\n",
      "******** Epoch 1000 took 6 seconds. ********\n",
      "loss : 0.00435248\n",
      "******** Epoch 2000 took 6 seconds. ********\n",
      "loss : 0.00358163\n",
      "******** Epoch 3000 took 6 seconds. ********\n",
      "loss : 0.00491239\n",
      "******** Epoch 4000 took 6 seconds. ********\n",
      "loss : 0.00650598\n",
      "******** Epoch 5000 took 6 seconds. ********\n",
      "loss : 0.00471992\n",
      "******** Epoch 6000 took 6 seconds. ********\n",
      "loss : 0.00214421\n",
      "******** Epoch 7000 took 6 seconds. ********\n",
      "loss : 0.00635441\n",
      "******** Epoch 8000 took 6 seconds. ********\n",
      "loss : 0.00140922\n",
      "******** Epoch 9000 took 6 seconds. ********\n",
      "loss : 0.00101446\n",
      "******** Epoch 10000 took 6 seconds. ********\n",
      "##### Cycle 1 : 68 seconds for sample generation. #####\n",
      "2000/2000 [==============================] - 0s\n",
      "# Real loss : 0.465018\n",
      "====================== Cycle 1 took 130 seconds. ======================\n",
      "====================== Cycle 2 started. ======================\n",
      "Batch size : 4000\n",
      "loss : 0.0168228\n",
      "******** Epoch 1000 took 12 seconds. ********\n",
      "loss : 0.0149861\n",
      "******** Epoch 2000 took 13 seconds. ********\n",
      "loss : 0.00871469\n",
      "******** Epoch 3000 took 12 seconds. ********\n",
      "loss : 0.0115996\n",
      "******** Epoch 4000 took 12 seconds. ********\n",
      "loss : 0.00746156\n",
      "******** Epoch 5000 took 12 seconds. ********\n",
      "loss : 0.00472809\n",
      "******** Epoch 6000 took 12 seconds. ********\n",
      "loss : 0.00944408\n",
      "******** Epoch 7000 took 12 seconds. ********\n",
      "loss : 0.0290616\n",
      "******** Epoch 8000 took 12 seconds. ********\n",
      "loss : 0.00738481\n",
      "******** Epoch 9000 took 12 seconds. ********\n",
      "loss : 0.00786092\n",
      "******** Epoch 10000 took 12 seconds. ********\n",
      "##### Cycle 2 : 322 seconds for sample generation. #####\n",
      "4000/4000 [==============================] - 0s\n",
      "# Real loss : 0.318083\n",
      "====================== Cycle 2 took 449 seconds. ======================\n",
      "====================== Cycle 3 started. ======================\n",
      "Batch size : 8000\n",
      "loss : 0.028479\n",
      "******** Epoch 1000 took 25 seconds. ********\n",
      "loss : 0.0263064\n",
      "******** Epoch 2000 took 25 seconds. ********\n",
      "loss : 0.0171577\n",
      "******** Epoch 3000 took 25 seconds. ********\n",
      "loss : 0.0093852\n",
      "******** Epoch 4000 took 25 seconds. ********\n",
      "loss : 0.00770902\n",
      "******** Epoch 5000 took 25 seconds. ********\n",
      "loss : 0.0197433\n",
      "******** Epoch 6000 took 25 seconds. ********\n",
      "loss : 0.00503619\n",
      "******** Epoch 7000 took 25 seconds. ********\n",
      "loss : 0.00630142\n",
      "******** Epoch 8000 took 25 seconds. ********\n",
      "loss : 0.02075\n",
      "******** Epoch 9000 took 25 seconds. ********\n",
      "loss : 0.00874437\n",
      "******** Epoch 10000 took 25 seconds. ********\n",
      "##### Cycle 3 : 1350 seconds for sample generation. #####\n",
      "8000/8000 [==============================] - 0s\n",
      "# Real loss : 0.391126\n",
      "====================== Cycle 3 took 1604 seconds. ======================\n",
      "====================== Cycle 4 started. ======================\n",
      "Batch size : 16000\n",
      "loss : 0.0139661\n",
      "******** Epoch 1000 took 50 seconds. ********\n",
      "loss : 0.00991813\n",
      "******** Epoch 2000 took 50 seconds. ********\n",
      "loss : 0.0352987\n",
      "******** Epoch 3000 took 49 seconds. ********\n",
      "loss : 0.00912442\n",
      "******** Epoch 4000 took 49 seconds. ********\n",
      "loss : 0.00957976\n",
      "******** Epoch 5000 took 50 seconds. ********\n",
      "loss : 0.00842678\n",
      "******** Epoch 6000 took 49 seconds. ********\n",
      "loss : 0.0210215\n",
      "******** Epoch 7000 took 50 seconds. ********\n",
      "loss : 0.0111906\n",
      "******** Epoch 8000 took 49 seconds. ********\n",
      "loss : 0.0101594\n",
      "******** Epoch 9000 took 49 seconds. ********\n",
      "loss : 0.00985292\n",
      "******** Epoch 10000 took 49 seconds. ********\n",
      "# Real loss : 0.279267\n",
      "====================== Cycle 4 took 501 seconds. ======================\n",
      "====================== Substitute Training time : 2728.6942529678345 seconds ======================\n"
     ]
    }
   ],
   "source": [
    "# Train the substitute with increased samples generated from Jacobian-based Augmentation\n",
    "\n",
    "# Determine the training parameters\n",
    "cycles, batch_size, epochs, lmbda, learning_rate = 5, 128, 10000, 0.1, 0.005\n",
    "\n",
    "# Only X0~X4 are continuous variables and therefore adjustable\n",
    "x = tf.placeholder(tf.float32, shape=[None, 18])\n",
    "y = tf.placeholder(tf.float32, shape=[None,1])\n",
    "y_pred = sub(x)\n",
    "prob = tf.sigmoid(y_pred)\n",
    "\n",
    "# Define the loss function\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels = y, logits = y_pred))\n",
    "\n",
    "# Define the training optimizer\n",
    "train_step = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Reload oracle weights due to previous initializer\n",
    "oracle.load_weights(\"model/oracle_weights.h5\")\n",
    "\n",
    "# Begin training\n",
    "t1 = time.time()\n",
    "with sess.as_default():\n",
    "    start, length = 0, len(output_sample)\n",
    "    permut = np.random.permutation(range(length))\n",
    "    batch_x, batch_y = sub_sample_x[permut], output_sample[permut]\n",
    "    grads = tf.gradients(prob, x) # 1-dim Jacobian matrix\n",
    "\n",
    "    for cycle in range(cycles):\n",
    "        length, t3 = len(batch_y), time.time()\n",
    "        print(\"====================== Cycle %d started. ======================\"%(cycle))\n",
    "        print(\"Batch size :\", length)\n",
    "        \n",
    "        # Begin training (epochs = 10000)\n",
    "        for epoch in range(epochs):\n",
    "            start = 0\n",
    "            permut = np.random.permutation(range(length))\n",
    "            batch_x, batch_y = batch_x[permut], batch_y[permut]\n",
    "            while True:\n",
    "                end = start + (batch_size - 1)\n",
    "                if end >= length:\n",
    "                    break\n",
    "                next_batch_x, next_batch_y = batch_x[start:end], batch_y[start:end]\n",
    "                train_step.run(feed_dict={x:next_batch_x, y:next_batch_y})\n",
    "                start += batch_size\n",
    "            if epoch % 1000 == 0:\n",
    "                t5 = time.time()\n",
    "            elif (epoch+1) % 1000 == 0:\n",
    "                print(\"loss :\", sess.run(loss, feed_dict={x:batch_x, y:batch_y}))\n",
    "                print(\"******** Epoch %d took %d seconds. ********\"%(epoch+1, time.time()-t5))\n",
    "\n",
    "        # Jacobian_based augmentation (lambda = 0.1), only adjust X0 ~ X4\n",
    "        if cycle < cycles -1 :\n",
    "            augmented_batch_x, t4 = [], time.time()\n",
    "            for batch in batch_x :\n",
    "                grad = sess.run(tf.sign(grads), feed_dict={x:batch.reshape(1,-1)}).reshape(-1,) ##\n",
    "                augmented_batch_x.append(batch + lmbda * np.hstack((grad[:5],np.zeros(13))))\n",
    "            print (\"##### Cycle %d : %d seconds for sample generation. #####\" %(cycle, time.time()-t4))\n",
    "            \n",
    "            augmented_batch_x = np.array(augmented_batch_x)\n",
    "            augmented_batch_y = oracle.predict(augmented_batch_x, batch_size=length, verbose=1)\n",
    "            augmented_batch_y = np.array([1 if num >= 0.5 else 0 for num in augmented_batch_y]).reshape(-1,1)\n",
    "            batch_x = np.vstack((batch_x, augmented_batch_x))\n",
    "            batch_y = np.vstack((batch_y, augmented_batch_y))\n",
    "        print(\"# Real loss :\", sess.run(loss, feed_dict={x:X_TRAIN, y:output_train}))\n",
    "        print(\"====================== Cycle %d took %d seconds. ======================\"%(cycle, time.time()-t3))\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"====================== Substitute Training time :\", t2-t1, \"seconds ======================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved substitute configuration to disk\n",
      "Saved substitute weights to disk\n",
      "Substitute saving time : 0.34546661376953125 seconds\n"
     ]
    }
   ],
   "source": [
    "# Saved substitute configuration and weights to disk with json and hdf respectively\n",
    "t1 = time.time()\n",
    "print(\"Saved substitute configuration to disk\")\n",
    "sub_json = sub.to_json()\n",
    "with open(\"model/sub.json\", \"w\") as sub_file:\n",
    "    sub_file.write(sub_json)\n",
    "\n",
    "print(\"Saved substitute weights to disk\")\n",
    "sub.save_weights(\"model/sub_weights.h5\")\n",
    "t2 = time.time()\n",
    "print (\"Substitute saving time :\", t2-t1, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of the oracle on the test set : [0 1 0 ..., 0 1 0]\n",
      "Accuracy of the oracle on the test set : 0.961653884628\n"
     ]
    }
   ],
   "source": [
    "# Test : Oracle\n",
    "output_test = oracle.predict(X_TEST, batch_size=2999, verbose=0)\n",
    "output_test = np.array([1 if num >= 0.5 else 0 for num in output_test])\n",
    "correct_list = np.array([1 if output == y else 0 for output,y in zip(output_test,Y_TEST)])\n",
    "print(\"Output of the oracle on the test set :\", output_test)\n",
    "print(\"Accuracy of the oracle on the test set :\", sum (correct_list)/2999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the substitute on the prediction of oracle output : 0.95975\n",
      "Accuracy of the substitute on the prediction of real output : 0.940166666667\n"
     ]
    }
   ],
   "source": [
    "# Test : Substitute\n",
    "prob = tf.sigmoid(sub(x))\n",
    "result = sess.run(prob, feed_dict={x:X_TRAIN})\n",
    "result = [1 if num > 0.5 else 0 for num in result]\n",
    "output = oracle.predict(X_TRAIN, batch_size=12000, verbose=0)\n",
    "output = np.array([1 if num >= 0.5 else 0 for num in output])\n",
    "correct_list = np.array([1 if output == y else 0 for output,y in zip(result, output)])\n",
    "print(\"Accuracy of the substitute on the prediction of oracle output :\",sum(correct_list)/12000)\n",
    "correct_list = np.array([1 if output == y else 0 for output,y in zip(result, Y_TRAIN)])\n",
    "print(\"Accuracy of the substitute on the prediction of real output :\",sum(correct_list)/12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast Gradient Sign Method based on L1 norm\n",
      "Adversarial samples generation took  788.4469368457794 seconds\n"
     ]
    }
   ],
   "source": [
    "# Initiate a blackbox attack with adversarial examples generating \n",
    "# from substitute model by Fast Gradient Sign Method (Default setting : L1 norm)\n",
    "\n",
    "# Define loss function of substitute model and calculate its gradients with respect to x\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels = y, logits = y_pred))\n",
    "grads = tf.gradients(loss, x)[0][0]\n",
    "\n",
    "# Set the parameter epsilon = 0.5 for FGSM (Only alter the first 5 terms , X0~X4)\n",
    "print (\"Fast Gradient Sign Method based on L1 norm\")\n",
    "t1 = time.time()\n",
    "epsilon, adv_X_TEST, adv_Y_TEST = 0.5, [], Y_TEST\n",
    "for i in range(len(X_TEST)):\n",
    "    grads_val = sess.run(grads, feed_dict={x:X_TEST[i].reshape(1,-1), y:Y_TEST[i].reshape(1,-1)})\n",
    "    grads_val = np.hstack((grads_val[:5], np.zeros(13)))\n",
    "    grads_l_1 = grads_val/(sum(abs(grads_val)))\n",
    "    grads_l_inf = sess.run(tf.sign(grads_val))\n",
    "    \n",
    "    aug = grads_l_1 # set aug = grads_l_inf if L-infinity norm is adopted \n",
    "    adv_X_TEST.append(X_TEST[i] + epsilon * aug)\n",
    "\n",
    "adv_X_TEST = np.array(adv_X_TEST)\n",
    "print (\"Adversarial samples generation took \", time.time()-t1, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of the oracle on the adversarial samples : [0 1 0 ..., 0 0 0]\n",
      "Accuracy of the oracle on the adversarial samples : 0.775591863955 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the accuarcy of the adversarial samples on the oracle\n",
    "output_test = oracle.predict(adv_X_TEST, batch_size=2999, verbose=0)\n",
    "output_test = np.array([1 if num >= 0.5 else 0 for num in output_test])\n",
    "correct_list = np.array([1 if output == y else 0 for output,y in zip(output_test,adv_Y_TEST)])\n",
    "print(\"Output of the oracle on the adversarial samples :\", output_test)\n",
    "print(\"Accuracy of the oracle on the adversarial samples :\", sum (correct_list)/2999, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of the oracle on the random augmented samples : [0 1 0 ..., 0 1 0]\n",
      "Accuracy of the oracle on the random augmented samples : 0.924308102701 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the accuarcy of the random augmented samples on the oracle\n",
    "rand_X_TEST, rand_Y_TEST = [], Y_TEST\n",
    "for i in range(len(X_TEST)):\n",
    "    rands = np.random.random_sample((5,))\n",
    "    symbols = np.array([-1.0,1.0])\n",
    "    unit, signs = rands/sum(rands), np.random.choice(symbols, 5)\n",
    "    aug_rand = np.hstack(([num * sign for num, sign in zip (unit,signs)], np.zeros(13)))\n",
    "    rand_X_TEST.append(X_TEST[i] + epsilon * aug_rand)\n",
    "rand_X_TEST = np.array(rand_X_TEST)\n",
    "\n",
    "output_test = oracle.predict(rand_X_TEST, batch_size=2999, verbose=0)\n",
    "output_test = np.array([1 if num >= 0.5 else 0 for num in output_test])\n",
    "correct_list = np.array([1 if output == y else 0 for output,y in zip(output_test,rand_Y_TEST)])\n",
    "print(\"Output of the oracle on the random augmented samples :\", output_test)\n",
    "print(\"Accuracy of the oracle on the random augmented samples :\", sum (correct_list)/2999, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total execution time : 3813.7235317230225 seconds\n"
     ]
    }
   ],
   "source": [
    "print (\"Total execution time :\", time.time()-t0, \"seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
